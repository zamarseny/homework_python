{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.sroboto.com/2017/09/pass-video-into-tensorflow-object.html\n",
    "#https://www.edureka.co/blog/tensorflow-object-detection-tutorial/\n",
    "#https://www.geeksforgeeks.org/saving-operated-video-from-a-webcam-using-opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!protoc -tensorflow_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow.__version__\n",
    "#nvidia-smi -l 2\n",
    "#tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    " \n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    " \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate\n",
    "from scipy.misc import imread, imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "#sys.path.append('/home/zamarseny/Data_Science/models/research/object_detection/')\n",
    "    \n",
    "from utils import label_map_util\n",
    " \n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3\n",
      "svmem(total=16734871552, available=7494963200, percent=55.2, used=8590450688, free=4407672832, active=9945190400, inactive=1722814464, buffers=280240128, cached=3456507904, shared=310243328, slab=261058560)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(psutil.cpu_percent())\n",
    "# gives an object with many fields\n",
    "print(psutil.virtual_memory())\n",
    "# you can convert that object to a dictionary \n",
    "dict(psutil.virtual_memory()._asdict())['percent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\tVersion\t\n",
    "MobileNet-SSD v1\t2017_11_17 \n",
    "MobileNet-SSD v1 PPN\t2018_07_03 \n",
    "MobileNet-SSD v2\t2018_03_29 \n",
    "Inception-SSD v2\t2017_11_17\n",
    "Faster-RCNN Inception v2\t2018_01_28 \n",
    "Faster-RCNN ResNet-50\t2018_01_28\n",
    "Mask-RCNN Inception v2\t2018_01_28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
    "#MODEL_NAME = 'mask_rcnn_inception_v2_coco_2018_01_28'\n",
    "\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "#DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "DOWNLOAD_BASE = '/home/zamarseny/Data_Science/models/research/object_detection/other_models/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opener = urllib.request.URLopener()\n",
    "#opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "#tar_file = tarfile.open(MODEL_FILE)\n",
    "tar_file = tarfile.open(DOWNLOAD_BASE + MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a (frozen) Tensorflow model into memory.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(min_score, boxes, scores, classes, categories):\n",
    "    \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n",
    "    n = len(classes)\n",
    "    idxs = []\n",
    "    for i in range(n):\n",
    "        if classes[0][i] in categories and scores[0][i] >= min_score:\n",
    "            idxs.append(i)\n",
    "    \n",
    "    filtered_boxes = boxes[idxs, ...]\n",
    "    filtered_scores = scores[idxs, ...]\n",
    "    filtered_classes = classes[idxs, ...]\n",
    "    return filtered_boxes, filtered_scores, filtered_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts 5, we know that this corresponds to airplane. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in category_index:\n",
    "    #if x in [11, 18, 17, 88]:\n",
    "    if x >1:\n",
    "        #print(x, category_index[x])\n",
    "        category_index[x]={'id' : x, 'name': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1, 'name': 'person'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=category_index[1]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'id': 1, 'name': 'person'},\n",
       " 2: {'id': 2, 'name': ''},\n",
       " 3: {'id': 3, 'name': ''},\n",
       " 4: {'id': 4, 'name': ''},\n",
       " 5: {'id': 5, 'name': ''},\n",
       " 6: {'id': 6, 'name': ''},\n",
       " 7: {'id': 7, 'name': ''},\n",
       " 8: {'id': 8, 'name': ''},\n",
       " 9: {'id': 9, 'name': ''},\n",
       " 10: {'id': 10, 'name': ''},\n",
       " 11: {'id': 11, 'name': ''},\n",
       " 13: {'id': 13, 'name': ''},\n",
       " 14: {'id': 14, 'name': ''},\n",
       " 15: {'id': 15, 'name': ''},\n",
       " 16: {'id': 16, 'name': ''},\n",
       " 17: {'id': 17, 'name': ''},\n",
       " 18: {'id': 18, 'name': ''},\n",
       " 19: {'id': 19, 'name': ''},\n",
       " 20: {'id': 20, 'name': ''},\n",
       " 21: {'id': 21, 'name': ''},\n",
       " 22: {'id': 22, 'name': ''},\n",
       " 23: {'id': 23, 'name': ''},\n",
       " 24: {'id': 24, 'name': ''},\n",
       " 25: {'id': 25, 'name': ''},\n",
       " 27: {'id': 27, 'name': ''},\n",
       " 28: {'id': 28, 'name': ''},\n",
       " 31: {'id': 31, 'name': ''},\n",
       " 32: {'id': 32, 'name': ''},\n",
       " 33: {'id': 33, 'name': ''},\n",
       " 34: {'id': 34, 'name': ''},\n",
       " 35: {'id': 35, 'name': ''},\n",
       " 36: {'id': 36, 'name': ''},\n",
       " 37: {'id': 37, 'name': ''},\n",
       " 38: {'id': 38, 'name': ''},\n",
       " 39: {'id': 39, 'name': ''},\n",
       " 40: {'id': 40, 'name': ''},\n",
       " 41: {'id': 41, 'name': ''},\n",
       " 42: {'id': 42, 'name': ''},\n",
       " 43: {'id': 43, 'name': ''},\n",
       " 44: {'id': 44, 'name': ''},\n",
       " 46: {'id': 46, 'name': ''},\n",
       " 47: {'id': 47, 'name': ''},\n",
       " 48: {'id': 48, 'name': ''},\n",
       " 49: {'id': 49, 'name': ''},\n",
       " 50: {'id': 50, 'name': ''},\n",
       " 51: {'id': 51, 'name': ''},\n",
       " 52: {'id': 52, 'name': ''},\n",
       " 53: {'id': 53, 'name': ''},\n",
       " 54: {'id': 54, 'name': ''},\n",
       " 55: {'id': 55, 'name': ''},\n",
       " 56: {'id': 56, 'name': ''},\n",
       " 57: {'id': 57, 'name': ''},\n",
       " 58: {'id': 58, 'name': ''},\n",
       " 59: {'id': 59, 'name': ''},\n",
       " 60: {'id': 60, 'name': ''},\n",
       " 61: {'id': 61, 'name': ''},\n",
       " 62: {'id': 62, 'name': ''},\n",
       " 63: {'id': 63, 'name': ''},\n",
       " 64: {'id': 64, 'name': ''},\n",
       " 65: {'id': 65, 'name': ''},\n",
       " 67: {'id': 67, 'name': ''},\n",
       " 70: {'id': 70, 'name': ''},\n",
       " 72: {'id': 72, 'name': ''},\n",
       " 73: {'id': 73, 'name': ''},\n",
       " 74: {'id': 74, 'name': ''},\n",
       " 75: {'id': 75, 'name': ''},\n",
       " 76: {'id': 76, 'name': ''},\n",
       " 77: {'id': 77, 'name': ''},\n",
       " 78: {'id': 78, 'name': ''},\n",
       " 79: {'id': 79, 'name': ''},\n",
       " 80: {'id': 80, 'name': ''},\n",
       " 81: {'id': 81, 'name': ''},\n",
       " 82: {'id': 82, 'name': ''},\n",
       " 84: {'id': 84, 'name': ''},\n",
       " 85: {'id': 85, 'name': ''},\n",
       " 86: {'id': 86, 'name': ''},\n",
       " 87: {'id': 87, 'name': ''},\n",
       " 88: {'id': 88, 'name': ''},\n",
       " 89: {'id': 89, 'name': ''},\n",
       " 90: {'id': 90, 'name': ''}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just show video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap = cv2.VideoCapture(0)\n",
    "#cap = cv2.VideoCapture('rtsp://192.168.0.188:544/profile1')\n",
    "cap = cv2.VideoCapture('rtsp://192.168.0.178:544/profile1')\n",
    "\n",
    "#rtsp://IP or domain name:port/profile1\n",
    "'''\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "    while True:\n",
    "        ret, image_np = cap.read()\n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        # Actual detection.\n",
    "        (boxes, scores, classes, num_detections) = sess.run(\n",
    "          [boxes, scores, classes, num_detections],\n",
    "          feed_dict={image_tensor: image_np_expanded})\n",
    "        # Visualization of the results of a detection.\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            np.squeeze(boxes),\n",
    "            np.squeeze(classes).astype(np.int32),\n",
    "            np.squeeze(scores),\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    " \n",
    "        cv2.imshow('object detection', cv2.resize(image_np, (800,600)))\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "          cv2.destroyAllWindows()\n",
    "          break\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save video captured from webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "out = cv2.VideoWriter('output.avi', fourcc, 5.0, (640, 480)) \n",
    "\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "\n",
    "    # output the frame. In while cycle \n",
    "    #out.write(gray)\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "  \n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "    while True:\n",
    "        ret, image_np = cap.read()\n",
    "        \n",
    "        #height , width , layers =  image_np.shape\n",
    "        #new_h=640#height/2\n",
    "        #new_w=480#width/2\n",
    "        #resize = cv2.resize(image_np, (new_w, new_h)) \n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        #image_np_expanded = np.expand_dims(resize, axis=0)\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        # Actual detection.\n",
    "        (boxes, scores, classes, num_detections) = sess.run(\n",
    "          [boxes, scores, classes, num_detections],\n",
    "          feed_dict={image_tensor: image_np_expanded})\n",
    "        # Visualization of the results of a detection.\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            np.squeeze(boxes),\n",
    "            np.squeeze(classes).astype(np.int32),\n",
    "            np.squeeze(scores),\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    " \n",
    "        cv2.imshow('object detection', cv2.resize(image_np, (800,600)))\n",
    "        #rgb = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "        hsv = cv2.cvtColor(image_np,cv2.COLOR_BGR2HSV)\n",
    "        #gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #out.write(gray)\n",
    "        out.write(image_np)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "          cv2.destroyAllWindows()\n",
    "          break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.0.0) /io/opencv/modules/imgproc/src/resize.cpp:3784: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-77c76bcd50e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#ret, frame2=cap2.read()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m480\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     '''(h, w) = frame.shape[:2]\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.0.0) /io/opencv/modules/imgproc/src/resize.cpp:3784: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "#cap = cv2.VideoCapture('rtsp://admin:123456@192.168.0.178:554/profile3')\n",
    "#cap = cv2.VideoCapture(0)\n",
    "#cap2 = cv2.VideoCapture('rtsp://192.168.0.188:554/profile3')\n",
    "\n",
    "cap = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/C0031.MP4')\n",
    "\n",
    "time_length = 37.0\n",
    "fps=50\n",
    "frame_seq = 37*50\n",
    "frame_no = (frame_seq /(time_length*fps))\n",
    "cap.set(2,frame_no);\n",
    "\n",
    "#The first argument of cap.set(), number 2 defines that parameter for setting the frame selection.\n",
    "#Number 2 defines flag CV_CAP_PROP_POS_FRAMES which is a 0-based index of the frame to be decoded/captured next.\n",
    "#The second argument defines the frame number in range 0.0-1.0\n",
    "#cap.set(2,frame_no);\n",
    "#cap = cv2.VideoCapture('rtsp://192.168.0.188:554/profile1')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "#out = cv2.VideoWriter('output.avi', fourcc, 50.0, (1920, 980)) \n",
    "out = cv2.VideoWriter('output.avi', fourcc, 50.0, (640, 480))\n",
    "#out2 = cv2.VideoWriter('output2.avi', fourcc, 30.0, (704, 1152))\n",
    "\n",
    "while True:#((cap.isOpened()) || (cap2.isOpened())):\n",
    "    ret, frame = cap.read()\n",
    "    #ret, frame2=cap2.read()\n",
    "    frame=cv2.resize(frame, (640, 480))\n",
    "    \n",
    "    '''(h, w) = frame.shape[:2]\n",
    "    # calculate the center of the image\n",
    "    center = (w / 2, h / 2)\n",
    " \n",
    "    angle90 = 90\n",
    "    angle270 = 270\n",
    "    scale = 1.0\n",
    " \n",
    "    # Perform the counter clockwise rotation holding at the center\n",
    "    # 90 degrees\n",
    "    M = cv2.getRotationMatrix2D(center, angle90, scale)\n",
    "    rotated90 = cv2.warpAffine(frame, M, (h, w))\n",
    "  \n",
    "    # 270 degrees\n",
    "    M = cv2.getRotationMatrix2D(center, angle270, scale)\n",
    "    rotated270 = cv2.warpAffine(frame2, M, (h, w))\n",
    "\n",
    "    #out_CW = frame.swapaxes(-2,-1)[...,::-1] # Clockwise\n",
    "    #out_CW=np.rot90(frame,axes=(-2,-1))\n",
    "    #out_CW = rotate(frame, 90)\n",
    "    #out_CW=cv2.flip(frame, 0)\n",
    "\n",
    "    #out_CCW = frame2.swapaxes(-2,-1)[...,::-1,:] # Counter-Clockwise\n",
    "    #out_CCW=np.rot90(frame2,axes=(-2,-1))\n",
    "    #out_CCW = cv2.flip(frame2, +1)\n",
    "    \n",
    "    vis = np.concatenate((rotated90, rotated270), axis=1)\n",
    "    '''\n",
    "    \n",
    "    #gray = cv2.cvtColor(vis, cv2.COLOR_BGR2GRAY)\n",
    "    #hsv = cv2.cvtColor(vis, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    out.write(frame)\n",
    "    #out2.write(out_CCW)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "#cap2.release()\n",
    "cv2.destroyAllWindows()\n",
    "#print(out_CW.shape, out_CCW.shape)\n",
    "#print(vis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 cams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 2112, 3)\n"
     ]
    }
   ],
   "source": [
    "cap1 = cv2.VideoCapture('rtsp://admin:123456@192.168.0.178:554/profile3')\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap2 = cv2.VideoCapture('rtsp://192.168.0.188:554/profile3')\n",
    "\n",
    "#cap = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/C0031.MP4')\n",
    "#cap1 = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/3_cams/178_big.mp4')\n",
    "#cap2 = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/3_cams/web.mp4')\n",
    "\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "#out = cv2.VideoWriter('output.avi', fourcc, 30.0, (640, 1200))\n",
    "\n",
    "while True:#((cap.isOpened()) and (cap2.isOpened()) and (cap1.isOpened())): #True:#\n",
    "    ret, frame = cap.read()\n",
    "    ret, frame1 = cap1.read()\n",
    "    ret, frame2=cap2.read()\n",
    "    frame=cv2.resize(frame, (704, 576))\n",
    "    \n",
    "    vis = np.concatenate((frame, frame1, frame2), axis=1)\n",
    "    #gray = cv2.cvtColor(vis, cv2.COLOR_BGR2GRAY)\n",
    "    hsv = cv2.cvtColor(vis, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    cv2.imshow('frame',vis)\n",
    "    #out.write(vis)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()\n",
    "#print(out_CW.shape, out_CCW.shape)\n",
    "print(vis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracker initialized!\n"
     ]
    }
   ],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "#cap = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/vertical_panorama_LQ.avi')\n",
    "#cap = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/3_cams/3_cams.mp4')\n",
    "cap = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/C0031.MP4')\n",
    "#fps = FPS().start()\n",
    "frm_nmb=0\n",
    "\n",
    "res_hei=480\n",
    "res_wid=640\n",
    "\n",
    "confidence_cutoff = 0.3\n",
    "people_class_id=1\n",
    "\n",
    "#cap1 = cv2.VideoCapture('rtsp://admin:123456@192.168.0.178:554/profile3')\n",
    "#cap1.set(CV_CAP_PROP_FPS, 10)\n",
    "#cap1.set(cv2.CAP_PROP_POS_MSEC,450)\n",
    "\n",
    "#cap = cv2.VideoCapture(0)\n",
    "#cap.set(CV_CAP_PROP_FPS, 10)\n",
    "#cap.set(cv2.CAP_PROP_POS_MSEC,450)\n",
    "\n",
    "#cap2 = cv2.VideoCapture('rtsp://192.168.0.188:554/profile3')\n",
    "#cap2.set(CV_CAP_PROP_FPS, 10)\n",
    "#cap2.set(cv2.CAP_PROP_POS_MSEC,450)\n",
    "\n",
    "\n",
    "#out = cv2.VideoWriter('output.avi', fourcc, 50.0, (res_wid, res_hei)) \n",
    "#out = cv2.VideoWriter('output.avi', fourcc, 30.0, (1152, 704))\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "\n",
    "    # output the frame. In while cycle \n",
    "    #out.write(gray)\n",
    "\n",
    "kcf_tracker = None  # объект трекера инициализируется при первой детекции\n",
    "\n",
    "    \n",
    "#cap = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/C0031.MP4')\n",
    "#cap = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/')\n",
    "\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "    while True:# ((cap.isOpened()) and (cap2.isOpened()) and (cap1.isOpened())): #\n",
    "        ret, frame = cap.read()\n",
    "        frm_nmb+=1\n",
    "        #ret, frame1 = cap1.read()\n",
    "        #ret, frame2=cap2.read()\n",
    "        #frame=cv2.resize(frame, (704, 576))\n",
    "     \n",
    "        #vis = np.concatenate((frame1, frame2\\\n",
    "                             #frame, \\\n",
    "                             #), axis=1)\n",
    "        #vis=cv2.resize(vis, (1056, 288))\n",
    "        vis=cv2.resize(frame, (res_wid, res_hei))\n",
    "    \n",
    "        \n",
    "        kcf_tracker_box = None  # результат работы трекера\n",
    "        \n",
    "        if kcf_tracker is not None:\n",
    "            # обновляем трекер и получаем результат трекинга\n",
    "            ok, box = kcf_tracker.update(vis)\n",
    "            # сохраняем результат трекинга\n",
    "            if ok:\n",
    "                kcf_tracker_box = box\n",
    "        '''  \n",
    "        '''      \n",
    "            \n",
    "        #height , width , layers =  image_np.shape\n",
    "        #new_h=640#height/2\n",
    "        #new_w=480#width/2\n",
    "        #resize = cv2.resize(image_np, (new_w, new_h)) \n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        #image_np_expanded = np.expand_dims(vis, axis=0)\n",
    "\n",
    "        \n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        image_np_expanded = np.expand_dims(vis, axis=0)\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        \n",
    "        scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        # Actual detection.\n",
    "        (boxes, scores, classes, num_detections) = sess.run(\n",
    "          [boxes, scores, classes, num_detections],\n",
    "          feed_dict={image_tensor: image_np_expanded})\n",
    "        # Visualization of the results of a detection.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "        boxes, scores, classes = filter_boxes(confidence_cutoff, boxes, scores, classes,[people_class_id])\n",
    "        \n",
    "        \n",
    "        \n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            vis,\n",
    "            np.squeeze(boxes),\n",
    "            np.squeeze(classes).astype(np.int32),\n",
    "            np.squeeze(scores),\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    "\n",
    "        \n",
    "        # инициализируем трекер первой детекцией\n",
    "        if len(boxes) != 0 and sum(boxes[0][0])>1 and classes[0][0]==1 and kcf_tracker is None:\n",
    "            kcf_tracker = cv2.TrackerKCF_create()\n",
    "            #(x, y, w, h) = boxes[0][0]\n",
    "            (x, y, w, h) =int(boxes[0][0][1]*res_wid), int(boxes[0][0][0]*res_hei), \\\n",
    "                int((boxes[0][0][3]-boxes[0][0][1])*res_wid), int((boxes[0][0][2]-boxes[0][0][0])*res_hei)\n",
    "            #(x, y, w, h) =(700, 400, 300, 280)\n",
    "            #(x, y, w, h) =(450, 150, 100, 140)\n",
    "            kcf_tracker.init(vis, (x,y,w,h))\n",
    "            print('tracker initialized!')\n",
    "            \n",
    "        # отрисовываем результат трекера\n",
    "        if kcf_tracker_box is not None: \n",
    "            (x, y, w, h) = map(int, kcf_tracker_box)\n",
    "            cv2.rectangle(vis, (x,y),(x+w,y+h), (0, 0, 255), 2)\n",
    "        '''\n",
    "        '''\n",
    "        #cv2.imshow('object detection', cv2.resize(image_np, (800, 600)))\n",
    "        cv2.putText(vis, \"CPU %: \" + str((psutil.cpu_percent())) \\\n",
    "                    +\", frame_N: \"+str(frm_nmb)\\\n",
    "                    +\", memory %: \" + str(dict(psutil.virtual_memory()._asdict())['percent'])\n",
    "                    #, (50,425, cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,50,100), 2);\n",
    "                    , (int(res_wid*0.1),int(res_hei*0.9)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,50,100), 2);\n",
    "        #cv2.putText(vis, \"memory %: \" + str(dict(psutil.virtual_memory()._asdict())['percent']), (50,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,50,50), 2);\n",
    "        \n",
    "        cv2.imshow('object detection', vis)\n",
    "        #rgb = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "        #hsv = cv2.cvtColor(image_np,cv2.COLOR_BGR2HSV)\n",
    "        #gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #out.write(gray)\n",
    "        #out.write(vis)\n",
    "        if (cv2.waitKey(25) & 0xFF == ord('q')):# or cap.isOpened()==False:\n",
    "            break\n",
    "cap.release()\n",
    "#cap1.release()\n",
    "#cap2.release()\n",
    "cv2.destroyAllWindows()\n",
    "#print(boxes[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(3)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5463956, 44.0, array([0.02024281, 0.18696493, 0.50290245, 0.67386603], dtype=float32))\n",
      "(0.5184626, 44.0, array([0.04522428, 0.03695059, 0.67587423, 0.5371871 ], dtype=float32))\n",
      "(0.4416754, 82.0, array([0.00708055, 0.54124033, 0.54639685, 0.8309562 ], dtype=float32))\n",
      "(0.3137143, 44.0, array([0.1374443 , 0.27328354, 0.2256522 , 0.31090522], dtype=float32))\n",
      "(0.31157836, 82.0, array([0.00974   , 0.01253679, 0.94280076, 0.95279276], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n",
      "(0.0, 1.0, array([0., 0., 0., 0.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "for i in zip(scores[0],classes[0], boxes[0]):\n",
    "    print(i)\n",
    "    #if i==num_detections:\n",
    "    #    break\n",
    "#classes[0]\n",
    "#boxes[0][0], \n",
    "#len(classes[0])\n",
    "#for i in num_detections:\n",
    "#    print(i)\n",
    "#for k in classes[0]:\n",
    "#    print(category_index[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 4)\n",
      "[[[0.14041132 0.40454316 0.9417222  0.5492838 ]\n",
      "  [0.         0.5540297  0.49081045 0.9122751 ]\n",
      "  [0.00920275 0.5404046  0.62717664 0.82933134]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(boxes.shape)\n",
    "#print(sum(boxes[0]))\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704, 1152)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_np[:,:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(kcf_tracker_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite( \"pic_from_vid.jpg\", vis );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "vis=cv2.imread(\"pic_from_vid.jpg\")\n",
    "res_hei=480\n",
    "res_wid=640\n",
    "\n",
    "#shape (1, 100, 4)\n",
    "boxes_sh=[[[0.35645306, 0.2802545,  0.993816,   0.5987009 ],\n",
    "  [0.13781881, 0.27505,    0.22558957, 0.3118625 ],\n",
    "  [0.34956592, 0.3624579,  0.63677037, 0.5153593 ],\n",
    "  [0.1486208,  0.2292244,  0.23650889, 0.25403312],\n",
    "  [0.00673261, 0.,         0.9445064,  0.9994811 ],\n",
    "  [0.01785691, 0.1654455,  0.45712894, 0.78636855],\n",
    "  [0.,         0.,         0.,         0.        ],\n",
    "  [0.,         0. ,        0.,         0.        ],\n",
    "  [0.,         0.,         0.,         0.        ],\n",
    "  [0.,         0.,         0.,         0.        ]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35645306, 0.2802545, 0.993816, 0.5987009]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(228, 134, 636, 287)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(boxes_sh[0][0])\n",
    "int(boxes_sh[0][0][0]*res_wid), int(boxes_sh[0][0][1]*res_hei), int(boxes_sh[0][0][2]*res_wid), int(boxes_sh[0][0][3]*res_hei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cv2.rectangle(image_np, (x,y),(x+w,y+h), (0, 0, 255), 2).shape)\n",
    "#cv2.rectangle(vis, (x,y),(x+w,y+h), (0, 0, 255), 2)\n",
    "for i in range(5):\n",
    "    cv2.rectangle(vis, (int(boxes_sh[0][i][1]*res_wid), int(boxes_sh[0][i][0]*res_hei)),\\\n",
    "        (int(boxes_sh[0][i][3]*res_wid), int(boxes_sh[0][i][2]*res_hei)), \\\n",
    "        (0, 0, 255), 2)\n",
    "    cv2.putText(vis, str(i), \\\n",
    "                    (int(boxes_sh[0][i][1]*res_wid), int(boxes_sh[0][i][0]*res_hei)), \\\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,50,50), 2);\n",
    "cv2.imshow('object detection', vis)\n",
    "\n",
    "#cv2.imwrite(\"my.png\",img)\n",
    "\n",
    "#cv2.imshow(\"lalala\", img)\n",
    "k = cv2.waitKey(0) # 0==wait forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228, 134)\n",
      "(636, 287)\n"
     ]
    }
   ],
   "source": [
    "print((int(boxes_sh[0][0][0]*res_wid), int(boxes_sh[0][0][1]*res_hei)))\n",
    "print((int((boxes_sh[0][0][2])*res_wid), int((boxes_sh[0][0][3])*res_hei)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
