{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "class CentroidTracker():\n",
    "    def __init__(self, maxDisappeared=50):\n",
    "        # initialize the next unique object ID along with two ordered\n",
    "        # dictionaries used to keep track of mapping a given object\n",
    "        # ID to its centroid and number of consecutive frames it has\n",
    "        # been marked as \"disappeared\", respectively\n",
    "        self.nextObjectID = 0\n",
    "        self.objects = OrderedDict()\n",
    "        self.disappeared = OrderedDict()\n",
    "\n",
    "        # store the number of maximum consecutive frames a given\n",
    "        # object is allowed to be marked as \"disappeared\" until we\n",
    "        # need to deregister the object from tracking\n",
    "        self.maxDisappeared = maxDisappeared\n",
    "    \n",
    "    def register(self, centroid):\n",
    "        # when registering an object we use the next available object\n",
    "        # ID to store the centroid\n",
    "        self.objects[self.nextObjectID] = centroid\n",
    "        self.disappeared[self.nextObjectID] = 0\n",
    "        self.nextObjectID += 1\n",
    "        \n",
    "    def deregister(self, objectID):\n",
    "        # to deregister an object ID we delete the object ID from\n",
    "        # both of our respective dictionaries\n",
    "        del self.objects[objectID]\n",
    "        del self.disappeared[objectID]\n",
    "        \n",
    "    def update(self, rects):\n",
    "        # check to see if the list of input bounding box rectangles\n",
    "        # is empty\n",
    "        if len(rects) == 0:\n",
    "            # loop over any existing tracked objects and mark them\n",
    "            # as disappeared\n",
    "            for objectID in self.disappeared.keys():\n",
    "                self.disappeared[objectID] += 1\n",
    " \n",
    "                # if we have reached a maximum number of consecutive\n",
    "                # frames where a given object has been marked as\n",
    "                # missing, deregister it\n",
    "                if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                    self.deregister(objectID)\n",
    " \n",
    "            # return early as there are no centroids or tracking info\n",
    "            # to update\n",
    "            return self.objects\n",
    "        \n",
    "        # initialize an array of input centroids for the current frame\n",
    "        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
    " \n",
    "        # loop over the bounding box rectangles\n",
    "        for (i, (startX, startY, endX, endY)) in enumerate(rects):\n",
    "            # use the bounding box coordinates to derive the centroid\n",
    "            cX = int((startX + endX) / 2.0)\n",
    "            cY = int((startY + endY) / 2.0)\n",
    "            inputCentroids[i] = (cX, cY)\n",
    "            \n",
    "        # if we are currently not tracking any objects take the input\n",
    "        # centroids and register each of them\n",
    "        if len(self.objects) == 0:\n",
    "            for i in range(0, len(inputCentroids)):\n",
    "                self.register(inputCentroids[i])\n",
    "                \n",
    "        # otherwise, are are currently tracking objects so we need to\n",
    "        # try to match the input centroids to existing object\n",
    "        # centroids\n",
    "        else:\n",
    "            # grab the set of object IDs and corresponding centroids\n",
    "            objectIDs = list(self.objects.keys())\n",
    "            objectCentroids = list(self.objects.values())\n",
    " \n",
    "            # compute the distance between each pair of object\n",
    "            # centroids and input centroids, respectively -- our\n",
    "            # goal will be to match an input centroid to an existing\n",
    "            # object centroid\n",
    "            D = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
    " \n",
    "            # in order to perform this matching we must (1) find the\n",
    "            # smallest value in each row and then (2) sort the row\n",
    "            # indexes based on their minimum values so that the row\n",
    "            # with the smallest value is at the *front* of the index\n",
    "            # list\n",
    "            rows = D.min(axis=1).argsort()\n",
    " \n",
    "            # next, we perform a similar process on the columns by\n",
    "            # finding the smallest value in each column and then\n",
    "            # sorting using the previously computed row index list\n",
    "            cols = D.argmin(axis=1)[rows]\n",
    "            \n",
    "            # in order to determine if we need to update, register,\n",
    "            # or deregister an object we need to keep track of which\n",
    "            # of the rows and column indexes we have already examined\n",
    "            usedRows = set()\n",
    "            usedCols = set()\n",
    " \n",
    "            # loop over the combination of the (row, column) index\n",
    "            # tuples\n",
    "            for (row, col) in zip(rows, cols):\n",
    "                # if we have already examined either the row or\n",
    "                # column value before, ignore it\n",
    "                # val\n",
    "                if row in usedRows or col in usedCols:\n",
    "                    continue\n",
    " \n",
    "                # otherwise, grab the object ID for the current row,\n",
    "                # set its new centroid, and reset the disappeared\n",
    "                # counter\n",
    "                objectID = objectIDs[row]\n",
    "                self.objects[objectID] = inputCentroids[col]\n",
    "                self.disappeared[objectID] = 0\n",
    " \n",
    "                # indicate that we have examined each of the row and\n",
    "                # column indexes, respectively\n",
    "                usedRows.add(row)\n",
    "                usedCols.add(col)\n",
    "                \n",
    "            # compute both the row and column index we have NOT yet\n",
    "            # examined\n",
    "            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
    "            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
    "            \n",
    "            # in the event that the number of object centroids is\n",
    "            # equal or greater than the number of input centroids\n",
    "            # we need to check and see if some of these objects have\n",
    "            # potentially disappeared\n",
    "            if D.shape[0] >= D.shape[1]:\n",
    "                # loop over the unused row indexes\n",
    "                for row in unusedRows:\n",
    "                    # grab the object ID for the corresponding row\n",
    "                    # index and increment the disappeared counter\n",
    "                    objectID = objectIDs[row]\n",
    "                    self.disappeared[objectID] += 1\n",
    " \n",
    "                    # check to see if the number of consecutive\n",
    "                    # frames the object has been marked \"disappeared\"\n",
    "                    # for warrants deregistering the object\n",
    "                    if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                        self.deregister(objectID)\n",
    "                        \n",
    "            # otherwise, if the number of input centroids is greater\n",
    "            # than the number of existing object centroids we need to\n",
    "            # register each new input centroid as a trackable object\n",
    "            else:\n",
    "                for col in unusedCols:\n",
    "                    self.register(inputCentroids[col])\n",
    " \n",
    "        # return the set of trackable objects\n",
    "        return self.objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82421549 0.32755369 0.33198071]\n",
      " [0.72642889 0.72506609 0.17058938]]\n",
      "[0.32755369 0.17058938]\n",
      "[1 0]\n",
      "[2 1]\n",
      "[(1, 2), (0, 1)]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "objectCentroids = np.random.uniform(size=(2, 2))\n",
    "centroids = np.random.uniform(size=(3, 2))\n",
    "D = dist.cdist(objectCentroids, centroids)\n",
    "print(D)\n",
    "print(D.min(axis=1))\n",
    "#array([0.32755369, 0.17058938])\n",
    "rows = D.min(axis=1).argsort()\n",
    "print(rows)\n",
    "D.argmin(axis=1)\n",
    "#array([1, 2])\n",
    "cols = D.argmin(axis=1)[rows]\n",
    "print(cols)\n",
    "#array([2, 1])\n",
    "\n",
    "print(list(zip(rows, cols)))\n",
    "#[(1, 2), (0, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original version (caffe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -p PROTOTXT -m MODEL [-c CONFIDENCE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -p/--prototxt, -m/--model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zamarseny/anaconda3/envs/nlp/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3299: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "#rom pyimagesearch.centroidtracker import CentroidTracker\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    " \n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "    help=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "    help=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "    help=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# initialize our centroid tracker and frame dimensions\n",
    "ct = CentroidTracker()\n",
    "(H, W) = (None, None)\n",
    " \n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    " \n",
    "# initialize the video stream and allow the camera sensor to warmup\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # read the next frame from the video stream and resize it\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    " \n",
    "    # if the frame dimensions are None, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    " \n",
    "    # construct a blob from the frame, pass it through the network,\n",
    "    # obtain our output predictions, and initialize the list of\n",
    "    # bounding box rectangles\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H),\n",
    "        (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    rects = []\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # filter out weak detections by ensuring the predicted\n",
    "        # probability is greater than a minimum threshold\n",
    "        if detections[0, 0, i, 2] > args[\"confidence\"]:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the object, then update the bounding box rectangles list\n",
    "            box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "            rects.append(box.astype(\"int\"))\n",
    "\n",
    "            # draw a bounding box surrounding the object so we can\n",
    "            # visualize it\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 255, 0), 2)\n",
    "            \n",
    "    # update our centroid tracker using the computed set of bounding\n",
    "    # box rectangles\n",
    "    objects = ct.update(rects)\n",
    " \n",
    "    # loop over the tracked objects\n",
    "    for (objectID, centroid) in objects.items():\n",
    "        # draw both the ID of the object and the centroid of the\n",
    "        # object on the output frame\n",
    "        text = \"ID {}\".format(objectID)\n",
    "        cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    " \n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    " \n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rewritten (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "#rom pyimagesearch.centroidtracker import CentroidTracker\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    " \n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    " \n",
    "from scipy.ndimage import rotate\n",
    "from scipy.misc import imread, imshow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 003-graphcut.ipynb\t\t        caffe_test.ipynb\r\n",
      " 003-superpixel.ipynb\t\t        detection_ex.ipynb\r\n",
      " 003-tracking_experiments.ipynb         hand\r\n",
      " 003-viola-jones.ipynb\t\t        output.avi\r\n",
      " 003-watershed.ipynb\t\t       'recognition classes.ipynb'\r\n",
      " another_tracking_from_internet.ipynb   Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!cd /home/zamarseny/Data_Science/models/research/object_detection/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_generators\r\n",
      "another_tracking.ipynb\r\n",
      "box_coders\r\n",
      "builders\r\n",
      "CONTRIBUTING.md\r\n",
      "core\r\n",
      "data\r\n",
      "data_decoders\r\n",
      "dataset_tools\r\n",
      "dockerfiles\r\n",
      "eval_util.py\r\n",
      "eval_util_test.py\r\n",
      "exporter.py\r\n",
      "exporter_test.py\r\n",
      "export_inference_graph.py\r\n",
      "export_tflite_ssd_graph_lib.py\r\n",
      "export_tflite_ssd_graph_lib_test.py\r\n",
      "export_tflite_ssd_graph.py\r\n",
      "faster_rcnn_inception_v2_coco_2018_01_28\r\n",
      "faster_rcnn_resnet50_coco_2018_01_28\r\n",
      "g3doc\r\n",
      "inference\r\n",
      "__init__.py\r\n",
      "inputs.py\r\n",
      "inputs_test.py\r\n",
      "legacy\r\n",
      "local_models.ipynb\r\n",
      "mask_rcnn_inception_v2_coco_2018_01_28\r\n",
      "matchers\r\n",
      "meta_architectures\r\n",
      "metrics\r\n",
      "model_hparams.py\r\n",
      "model_lib.py\r\n",
      "model_lib_test.py\r\n",
      "model_main.py\r\n",
      "models\r\n",
      "model_tpu_main.py\r\n",
      "object_detection_tutorial.ipynb\r\n",
      "other_models\r\n",
      "output.avi\r\n",
      "pic_from_vid.jpg\r\n",
      "predictors\r\n",
      "protos\r\n",
      "__pycache__\r\n",
      "README.md\r\n",
      "samples\r\n",
      "ssd_mobilenet_v1_coco_11_06_2017\r\n",
      "ssd_mobilenet_v1_coco_11_06_2017.tar.gz\r\n",
      "ssd_mobilenet_v1_coco_2017_11_17\r\n",
      "ssd_mobilenet_v1_coco_2017_11_17.tar.gz\r\n",
      "ssd_mobilenet_v2_coco_2018_03_29\r\n",
      "test_ckpt\r\n",
      "test_data\r\n",
      "test_images\r\n",
      "Traffic-Light-Detection.ipynb\r\n",
      "trax.ipynb\r\n",
      "Untitled.ipynb\r\n",
      "utils\r\n",
      "working111.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd /home/zamarseny/Data_Science/models/research/object_detection/\n",
    "sys.path.append(\"..\")\n",
    "#sys.path.append('/home/zamarseny/Data_Science/models/research/object_detection')\n",
    "\n",
    "#sys.path.insert(0, '/home/zamarseny/Data_Science/models/research/object_detection/utils')\n",
    "\n",
    "#import file\n",
    "\n",
    "from utils import label_map_util\n",
    " \n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "#import psutil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
    "#MODEL_NAME = 'mask_rcnn_inception_v2_coco_2018_01_28'\n",
    "\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "#DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "DOWNLOAD_BASE = '/home/zamarseny/Data_Science/models/research/object_detection/other_models/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "#opener = urllib.request.URLopener()\n",
    "#opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "#tar_file = tarfile.open(MODEL_FILE)\n",
    "tar_file = tarfile.open(DOWNLOAD_BASE + MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())\n",
    "    \n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(min_score, boxes, scores, classes, categories):\n",
    "    \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n",
    "\n",
    "    n = len(classes[0])\n",
    "    #print(n)\n",
    "    idxs = []\n",
    "    filtered_boxes=[]\n",
    "    filtered_scores=[]\n",
    "    filtered_classes=[]\n",
    "    for i in range(n):\n",
    "        if classes[0][i] in categories and scores[0][i] >= min_score:\n",
    "            idxs.append(i)\n",
    "            filtered_boxes.append(boxes[0][i])\n",
    "            filtered_scores.append(scores[0][i])\n",
    "            filtered_classes.append(classes[0][i])\n",
    "    \n",
    "    return np.array([filtered_boxes]), np.array([filtered_scores]), np.array([filtered_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "    help=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "    help=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "    help=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "'''\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG') \n",
    "\n",
    "frm_nmb=0\n",
    "\n",
    "res_hei=480#360#720 #360 #288#576 \n",
    "res_wid=640\n",
    "\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (res_wid, res_hei)) \n",
    "\n",
    "confidence_cutoff = 0.4\n",
    "interesting_classes =[1]\n",
    "start_time=datetime.now()\n",
    "\n",
    "\n",
    "# initialize our centroid tracker and frame dimensions\n",
    "ct = CentroidTracker()\n",
    "(H, W) = (None, None)\n",
    " \n",
    "'''    \n",
    "# initialize the video stream and allow the camera sensor to warmup\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "'''\n",
    "\n",
    "#Substitute for Video Stream\n",
    "#cap = cv2.VideoCapture('rtsp://admin:68gh8h948fh@109.188.95.8:2983/cam/realmonitor?channel=6&subtype=1')\n",
    "cap = cv2.VideoCapture('/home/zamarseny/Data_Science/input_data/MOT_Challenge_Visualize.mp4')\n",
    "cap = cv2.VideoCapture('horizontal_panorama.avi'\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "#while True: #cap.isOpened(): #True\n",
    "    # read the next frame from the video stream and resize it\n",
    "    #frame = vs.read()\n",
    "\n",
    "with detection_graph.as_default():\n",
    "  #with tf.Session(graph=detection_graph) as sess:\n",
    "  with tf.Session(graph=detection_graph\\\n",
    "                  #, config=tf.ConfigProto(gpu_options=gpu_options)\\\n",
    "                 ) as sess:\n",
    "    while  (cap.isOpened()):\n",
    "    \n",
    "        #Substitute for Video Stream\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #frame = imutils.resize(frame, width=400)\n",
    "        frame=cv2.resize(frame, (res_wid, res_hei))\n",
    " \n",
    "        # if the frame dimensions are None, grab them\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    " \n",
    "        vis=frame\n",
    "    \n",
    "        image_np_expanded = np.expand_dims(vis, axis=0)\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        \n",
    "        cur_time=datetime.now()#.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        d=cur_time-start_time\n",
    "        #d.seconds\n",
    "        \n",
    "        scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        # Actual detection.\n",
    "        (boxes, scores, classes, num_detections) = sess.run(\n",
    "          [boxes, scores, classes, num_detections],\n",
    "          feed_dict={image_tensor: image_np_expanded})\n",
    "        \n",
    "        # Visualization of the results of a detection.\n",
    "        # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "        boxes, scores, classes = filter_boxes(confidence_cutoff, boxes, scores, classes, interesting_classes)\n",
    "        \n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            vis,\n",
    "            np.squeeze(boxes),\n",
    "            np.squeeze(classes).astype(np.int32),\n",
    "            np.squeeze(scores),\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=2)\n",
    "\n",
    "        '''\n",
    "        # construct a blob from the frame, pass it through the network,\n",
    "        # obtain our output predictions, and initialize the list of\n",
    "        # bounding box rectangles\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H),\n",
    "            (104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        rects = []\n",
    "    \n",
    "        '''\n",
    "\n",
    "    \n",
    "        '''\n",
    "        # loop over the detections\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            # filter out weak detections by ensuring the predicted\n",
    "            # probability is greater than a minimum threshold\n",
    "            if detections[0, 0, i, 2] > conf #args[\"confidence\"]:\n",
    "                # compute the (x, y)-coordinates of the bounding box for\n",
    "                # the object, then update the bounding box rectangles list\n",
    "                box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "                rects.append(box.astype(\"int\"))\n",
    "\n",
    "                # draw a bounding box surrounding the object so we can\n",
    "                # visualize it\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                    (0, 255, 0), 2)\n",
    "        '''    \n",
    "        # update our centroid tracker using the computed set of bounding\n",
    "        # box rectangles\n",
    "        multipl=np.array([[res_wid, 0, 0, 0], [0, res_hei, 0, 0 ], [0, 0, res_wid, 0], [0, 0, 0, res_hei]])\n",
    "        rects=np.array(np.matmul(boxes[0], multipl), int)\n",
    "\n",
    "        objects = ct.update(rects) #rects\n",
    " \n",
    "        # loop over the tracked objects\n",
    "        for (objectID, centroid) in objects.items():\n",
    "            # draw both the ID of the object and the centroid of the\n",
    "            # object on the output frame\n",
    "            text = \"ID {}\".format(objectID)\n",
    "            cv2.putText(vis, text, (centroid[0] - 10, centroid[1] - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "            cv2.circle(vis, (centroid[0], centroid[1]), 6, (0, 0, 255), -1)\n",
    "        ''' \n",
    "        '''\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", vis)\n",
    "        out.write(vis)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    " \n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    " \n",
    "# do a bit of cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis.shape[:2]\n",
    "\n",
    "(left, right, top, bottom) = (xmin * im_width, xmax * im_width, \n",
    "                              ymin * im_height, ymax * im_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f78f2d29c3f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmultipl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultipl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultipl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boxes' is not defined"
     ]
    }
   ],
   "source": [
    "print(boxes.shape)\n",
    "print(boxes[0])\n",
    "multipl=np.array([[100, 0, 0, 0], [0, 200, 0, 0 ], [0, 0, 100, 0], [0, 0, 0, 200]])\n",
    "print(multipl)\n",
    "rect=np.array(np.matmul(boxes[0], multipl), int)\n",
    "rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ab9ed2dc3d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mymin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres_hei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres_wid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres_hei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres_wid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boxes' is not defined"
     ]
    }
   ],
   "source": [
    "ymin = (boxes[0,:,0]*res_hei)\n",
    "xmin = (boxes[0,:,1]*res_wid)\n",
    "ymax = (boxes[0,:,2]*res_hei)\n",
    "xmax = (boxes[0,:,3]*res_wid)\n",
    "\n",
    "#print(ymin)\n",
    "\n",
    "print(np.array(xmin, int))\n",
    "rect=np.concatenate(np.array(ymin, int), np.array(xmin, int))\n",
    "rect=np.concatenate(rect, np.array(ymin, int))\n",
    "rect=rect=np.concatenate(rect, np.array(ymin, int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, array([186, 217])),\n",
       "             (2, array([163, 211])),\n",
       "             (4, array([308, 236])),\n",
       "             (5, array([180, 233])),\n",
       "             (7, array([200, 143])),\n",
       "             (8, array([150, 341])),\n",
       "             (9, array([128, 287])),\n",
       "             (10, array([226, 117]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5540281]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-397d543883c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
